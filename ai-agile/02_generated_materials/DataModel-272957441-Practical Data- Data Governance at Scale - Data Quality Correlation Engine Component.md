---
source: https://maoperatingsystem.atlassian.net/spaces/DataModel/pages/272957441/Practical+Data+Data+Governance+at+Scale+-+Data+Quality+Correlation+Engine+Component
confluence_id: 272957441
space: DataModel
version: 96
retrieved: 2026-01-23T12:37:32.095674
format: xhtml
content_hash: 5a80f0c008a3ea6110a18e1ebe58984d
macro_structured_macro: 24
macro_image: 0
macro_link: 0
---
About this DocumentThis document is part of a series on a target reference architecture for Data Governance at scale, designed for large, complex organizations. It provides a high-level overview definition of theData Quality Correlation Engine Component, a key element of the proposed architecture.The document focuses on the correlation engine as a “Rules Management platform”.  We expect a future iteration of the platform to be AI-based, leveraging patterns in the input stream and the resulting exception work stream (e.g., how many work items are cancelled or resolve themselves).  We recommend implementing the first couple of phases of this product as a rules engine, then deciding whether and how to evaluate AI solutions.This document is not intended to list the individual live business requirements for each correlation rule; instead, it provides the initial foundational design, including a set of correlation rule categories that are expected to arise in live operations.As a next step, in addition to this document, we recommend inventorying the current correlation rules (explicit or implied) within any Data Quality Engine or Work Management Engine, and validating the list of rule categories below against the inventory for completeness.We also recognize that the exact implementation and sequencing of different capabilities will depend on technology implementation and business-rule-specific prioritization.  In the roadmap section below, we recommend including a bare-bones “one-in-one-out” correlation engine in the day-1 stack as a starting point.Table of ContentsnoneThis document describes various design elements, such as Correlation Rule Language and Data Models, included here for communication and understanding only.  The exact technical specifications for these elements will need to be confirmed during detailed project implementation.Data Quality Correlation Engine ComponentBackgroundOverviewThe Data Quality Correlation Engine Component is designed to act as a central hub for consuming live data quality issues as they are published by various data quality assessment engines across the enterprise. Its primary function is to evaluate incoming issues and, based on predefined rules or AI, determine the required exception-management work to resolve them and initiate the appropriate downstream processes. This can include triggering an automated Data Quality Workflow for remediation or creating a Data Concern management case for further investigation. Over time, this rules-based approach will be enhanced with artificial intelligence capabilities to enable more sophisticated, predictive, and automated responses to data quality events.21002770862312729574411Untitled Diagram-1767044883397.drawio24https://maoperatingsystem.atlassian.net/wikiUntitled Diagram-1767044883397.drawio01387.5525Business Value of a Correlation EngineA dedicated Correlation Engine is the critical link that translates technical data quality noise into actionable business intelligence. By implementing this capability, an organization can achieve the following outcomes:Centralized, transparent management of decision rules that define how data quality issues should be handled.  Rules are no longer “hidden” in separate DQ Engines or work management engines for suppressing, consolidating, or prioritizing different matters.Prioritize Business Impact:Move the conversation from "we have 10,000 data errors" to "we have three critical data issues impacting regulatory reporting." The engine correlates technical issues with business processes and data lineage, ensuring resources are focused on fixing what matters most.Reduce Operational Costs:Automatically deduplicate redundant alerts and suppress low-impact issues, significantly reducing manual triage effort. This frees data professionals to focus on root-cause analysis rather than administrative overhead.Enable Proactive Risk Management:Identify systemic patterns invisible in individual errors. By connecting seemingly unrelated issues, the engine can anticipate and flag high-risk scenarios, allowing the organization to move from a reactive to a proactive risk posture.Improve Decision-Making:With prioritized, context-rich insights, business leaders can make more confident decisions based on reliable data for strategic planning and financial reporting.Provide a Foundation for Scalable Automation:As the data landscape grows, the engine becomes the foundational component for intelligent automation, driving a scalable data quality framework without a linear increase in headcount.Key Definitions / GlossaryTermDefinitionExampleData Quality Issue (DQI)A specific, recorded instance of a data anomaly or failure against a predefined data quality rule. As a minimum, a unique Data Quality Issue is defined as the combination ofData Quality Rule + Source System + Source System Record ID + Source System Field IDA missing value in a required field, an invalid format, or a value outside its acceptable range.Exception Work Item (EWI)A predefined, automated, or manual task initiated in response to a data quality issue. This could range from simple notifications to complex remediation workflows.Creating a Jira ticket for a data steward to investigate a failed validation, or triggering an automated email notification to a system owner.Correlation RuleA configurable instruction within the Correlation Engine that defines the conditions under which one or more data quality issues should be grouped, prioritized, suppressed, or escalated, and where to send the resulting action.IF more than 100 address validation errors occur in the CRM system within 1 hour, THEN create a single 'CRITICAL' priority ticket for the 'Data Engineering' team.Correlation Engine (CE)The system described in this document determines which Data Quality Actions are required in response to one or more Data Quality Issues.Data Quality Engine (DQE)A system responsible for executing data quality rules against data sets to identify and report data quality issues.An Informatica Data Quality instance that runs daily profiles on the Customer Master database, or a custom Python script that validates records as they stream into a data lake.Exception Work Management Engine (EME)A system that manages the lifecycle of data quality issues, including the assignment, tracking, and resolution of remediation tasks (Data Quality Work Actions).DIMR System (Data Issues for Management Reporting)A system responsible for collecting and reporting data quality metrics and issue statuses for management oversight, trend analysis, and governance reporting.Business Source SystemAn underlying source system where the data quality issue exists that has delegated Data Quality Execution to a specific Data Quality Engine.Data ConcernA formal record is created when a data quality issue or a pattern of topics requires manual investigation and business-level context to resolve, often because the root cause is not immediately apparent or spans multiple systems.Data LineageThe documented path of data from its origin to its destination, including all transformations and processing steps along the way. It provides context for understanding data dependencies and the impact of quality issues.Critical Data Element (CDE)A data element that has been formally designated as critical to a key business process, report, or regulatory requirement. CDEs are typically subject to the highest level of data quality monitoring and governance.RemediationThe process of correcting data quality issues at their source to prevent recurrence. This can involve manual data-entry corrections, fixing bugs in application code, or improving data-integration processes.Data Quality Issue to Exception Work Item LifecycleAs part of establishing a common correlation engine component, we have defined a common lifecycle approach to map the Data Quality Issue Lifecycle to the Exception Work Item Lifecycle.   Establishing this design is fundamental to a working correlation engine that governs how Exception Work Events are created in response to Data Quality Events.For a correlation engine, we identify two implementation patterns and three sequencing scenarios that need to be supported.Source Events fully control destination events - i.e., A destination event can only be closed as a result of a source closure event.  In this implementation, Exception Work Management events can only be fully CLOSED when a PASS event is received from the Data Quality Engine.Destination Events can be managed independently - i.e., A destination event can be managed (ie, closed) independently of source event status.   In this situation, we introduce a “Validated” state within the Data Quality Workflow Engines. This results in two scenarios.Work is RESOLVED in the Work Management Engine, and the Data Quality Engine then validates it in the next cycle.The Data Quality Engine validates the issue resolution before the Work Item Event is RESOLVED.  It is vital to detect this situation so we can evolve rules over time and iteratively refine work items that ultimately resolve the Scenario automatically without intervention.bottomScenario 1 – PASS received BEFORE RESOLVEDfit4bottomScenario 2 – PASS received AFTER RESOLVEDfit4bottomtitle Implementation 2 – Scenario 2 - PASS received BEFORE RESOLVEDfit2Data Sensitivity and Information SecurityTo assess the initial component design and to understand data sensitivity and security risks, we have classified the data elements as follows. The data sensitivity classification of the correlation engine as a whole will change based on the integration and implementation roadmap.Data ContentDescriptionProposed ClassificationCommentsData Quality Issue Information (DQI)Data Quality Issue Information is defined as metadata indicating that a data quality issue exists in a specific system, on a particular row, in a specific field. We explicitly define a Data Quality Issue as data that does not contain the actual content of the data records being evaluated by the data quality engine.INTERNAL  / NON-SENSITIVEBusiness Record Information (BRI)Separately from Data Quality issue Information, more advanced correlation rules may require (limited) data content from the business content evaluated in Data Quality Engines. Within this document, we have separated DQI from BRI to facilitate a phased implementation.SENSITIVE(Depending on the business system)Opportunity to delay business record information integrations to later phases of the project, depending on business rule capability prioritization.  This will allow the initial stages of the project to be implemented using an “INTERNAL” data classification.Correlation RulesThe correlation rule specifies how data quality issues should be managed via work items or other escalation processes.INTERNALException Work Information (EWI)Exception work information is defined here as information describing who and how the organization needs to respond to a data quality issue.In general, the EWI information will also contain details of the DQI.Depending on individual rule requirements, it may also include BRI details and therefore inherit BRI sensitivity.INTERNAL(Depending on the business system)EWI classification will follow the BRI classification.Business Use CasesBelow are sample business use cases illustrating the initial design of the correlation engine rules. The engine's objective is to provide an infinitely scalable solution for building any rule that describes how data quality alerts should be interpreted to determine when, who, and how to escalate work.  It is not expected that all of these features will be delivered on day one. They will be delivered incrementally, and additional capabilities are expected to be identified and incorporated.Use Case 1: Regulatory Reporting Accuracy.A series of minor, unrelated data validation errors is detected in an upstream customer onboarding system. Individually, they are low priority. The Correlation Engine, using data lineage, identifies that these records are all flowing into a critical risk calculation for a regulatory report (e.g., BCBS 239). The engine consolidates these individual alerts into a single, high-priority incident, flags the potential impact on the regulatory filing, and assigns it directly to the data stewardship team responsible for the risk platform, bypassing normal Level 1 support.Use Case 2: Customer Billing Integrity.A data quality scan of the billing database identifies a 5% increase in address-validation failures for new customers. Simultaneously, the engine receives an alert that a specific API endpoint for a new marketing campaign is experiencing intermittent timeouts. The Correlation Engine links these two events, hypothesizes that the faulty API is creating partial customer records, and automatically generates a "Data Concern" case. It assigns the case to both the marketing operations team and the billing data stewards, providing the context that a faulty marketing integration is likely to cause a spike in incorrect customer bills.AssumptionsCorrelation Engine Design AssumptionsIn establishing the high-level design for the Correlation Engine component, we have made the following assumptions.By definition, Correlation Rules determine when, where, and how Data Quality Issues need to be escalated and "worked." Correlation rules do not define what a Data Quality issue is - Just how to handle it once it is identified.Correlation rules are mutually independent from each other, rule order does not matter, and any rule can operate in parallel to another. SeeScalingbelow.The correlation engine itself is not authoritative for the state; instead, the Data Quality Engines are authoritative for the current state of data quality issues, and the Work Management Engines are authoritative for the current state of work activities. However, for implementation and performance reasons, the data quality engine should have an internal “near-time” understanding of these.A single Data Quality Exception can result in zero, one, or more Data Quality Work items/Exceptions, depending on the matching rules.For correlation, we define a single Data Quality event as the unique combination of RuleID, SystemID, RecordID, and FieldID.Correlation Rules are managed alongside data quality rules within the Data Governance Metadata store.Data Quality Engine Design AssumptionsIn establishing the design for the correlation engine component, we have also made several design assumptions about the data quality engines and the available data quality issue information.Data Quality engines can send their results as real-time issues or as batched results by Job.A single Data Quality Issue from a DQ engine may contain multiple “issues.” For example, many records share the same empty field, or a single record has multiple issues.  For the purposes of evaluating correlation rules, these will need to be decomposed and treated as single events (i.e., broken down into individual issue events).To understand the current state of data quality, the correlation engine needs to include capabilities to “match” subsequent PASS events to previous FAIL events associated with the same RuleID+SystemID+RecordID+FieldID. Combination.Not all Data Quality Engines will support the explicit publication of a “PASS” event.   Data Quality Engines that do not support explicit passes must associate issues with jobs. The correlation engine will need a capability to infer a successful PASS if the same problem no longer occurs in subsequent jobs, passes, etc.Data Quality Issues can be identified by a universal unique key across all Data Quality Engines, rather than being engine-specific.  This ID may be a compound ID, i.e., SystemA: Issue1, i.e., $DQI.ID.Logical and physical data element definitions, along with lineage information, already exist in an appropriate data inventory and lineage metadata store, which can be leveraged to correlate issues across multiple points in a data pipeline.Exception Work Management Engine Design AssumptionsIn establishing the design for the correlation engine component, we have also made several design assumptions about the Exception Work Management Engines and the available data quality issue information.Exception Work Items can be identified by a universal unique key across all Exception Work engines and not specific to individual engines. This ID may be a compound ID, i.e., SystemA: WorkItem1, i.e., $EWI.ID.Exception Work Management Engines can accept information about one or more data quality issues - i.e., A single Exception Workitem ticket can relate to many Data quality issues, usually as multiple fields on the same record or multiple records with the same field issue.Work Item state can be standardized through the integration to align to a standardized state that can be understood by the correlation engine - i.e., NEW→ IN PROGRESS → RESOLVED → VALIDATED | CANCELLED.If exception engines need access to Business Record Information to aid issue resolution, they can access it directly via an appropriate integration.  Business Record information will not be passed from the Correlation Engine to the Work Flow engine; only information about the Data Quality issue will be passed.Within Exception Work Management Tools, you can initiate work by assigning it to a specific "assignment group" or by creating a specific "work item type". The correlation engines must support both work-management approaches.Product DesignThis section outlines the high-level product design of the Correlation Engine and is intended for initial product scoping. Exact product details and specifications must be validated during implementation. An initial high-level product roadmap is also included below.Sample Data Model DesignThe data models below describe the correlation engine component and the sample rules. We have also included a baseline definition of key data elements.  Exact data models and structures will need to be designed and validated as part of project implementations.Correlation Rule Data Model Design ($CE)To describe the Correlation Engine component and an initial set of key data points for rules, we define the following minimum set of attributes as part of the Correlation Rule Data Model.  These are intended to support proper governance, auditability, and operational management, with each rule in the engine treated as a managed asset with its own metadata and lifecycle. This ensures that the rule base remains transparent, maintainable, and aligned with business objectives.  This definition also includes a plain-English description of the rule and its formal specification.AttributeDescriptionSourceComments1IDThe unique ID of the Correlation Engine Rule2OwnerThe business or technical team responsible for the rule's logic and maintenance.CE3StatusThe current state of the rule (e.g.,Draft, Test, Active, Inactive, Archived).CE4CreateThe timestamp the rule was first createdCE5ModifiedThe timestamp the rule was last modifiedCE6ModifiedByThe individual who last modified the ruleCE7DescriptionA plain English, human-readable description of the rule’s intent and purpose.CE8SpecificationThe exact formal technical specification of the rule using the formal rules language (see below)CE9VersionHistoryA full audit trail of all rule changes to the rule over time.CEData Quality Issue ($DQI) Data Model Design for Correlation EngineThe Correlation Engine consumes Data Quality Issues (DQIs) generated by upstream Data Quality Engines. This section describes a minimal internal data model for managing Data Quality Information within the Correlation Engine.  This is included solely to communicate concepts and document the sample rules below.  The exact complete payload definition will be finalized as part of the Data Quality Engine Project and Data Quality Engine Integrations.  These fields become implicit variables available within the Correlation Rules syntax.In the Data Quality Correlation Engine operating model, we define unique Data Quality Issue events as the compound key HASH(DQRuleID + SourceSystemID + FieldID + RecordID).  Within the correlation engine, this becomes the default compound key to match consecutive Data Quality FAIL and PASS events.FieldDescription & PurposeSourceExample ValueAdditional Comments1IDUnique identifier for this DQI, generated by the source Data Quality Engine. Must be globally unique and immutable across multiple data quality engines.  Used for deduplication and audit trail linkage.DQEDQI-20260108-SAP-INV-001-5847Globally Unique across all DQ Engines.2TimestampTimestamp when the DQI was generated by the source Data Quality Engine, in UTC. Used for time-window-based functions (COUNTISSUESINPERIOD) and audit trail sequencing.DQE2026-01-08T14:30:45.123Z3StateState of the event coming from the DQ engine - ie, FAIL or PASS.   Note that we expect most engines to send only FAIL events.  The correlation engine should be capable of inferring PASS events when no fail event occurs in the next run.DQEPASS or FAIL4JobIDUnique ID/Code from the correlation Engine that represents a single run/job/cycle.  Used to infer corresponding PASS events from consecutive jobs.DQEQDJ-20260108-SAP-12345DQEngineIDUnique ID/Code of the Data Quality Engine that generated this DQI.  Must match the system registry in Data Inventory.DQETalend6SourceSystemUnique ID/Code of the underlying source business system where the data quality issue originates.  Must match the system registry in Data Inventory.DQESalesforce7DQRuleIDUnique ID/Code of the Data Quality rule that failed, as defined in the source Data Quality Engine. Must be registered in the Data Quality Rule inventory.DQEDQRID:1234567898FieldIDThe Unique ID/Code of the physical data element field in the source system where the quality issue exists.  Must match the system registry in Data Inventory.DQEGFCID.ID9RecordIDThe Unique ID/Code of the business record in the source system that failed. Ideally, this should be defined as the business identifier to enable implied lineage correlation across multiple systems.DQEGFCID:12345678910MatchKey(Computed)Calculated dynamically within the correlation engine as a default, compound unique key to represent a unique data quality issue within the ecosystem at any one point in time. computed as HASH(DQ Rule ID + Source System ID + Field ID + Record ID).CECUST-001:SAP: Credit_Score:5847 (hash of components)11MatchedRuleCount(Computed)Calculated dynamically within the correlation engine, the dynamic count of the number of correlation engine rules that have positively matched against an individual data quality issue.  Useful for understanding if any other rules have been triggered so far.  Note that rules order does not matter.CE12MatchedWorkItems(Computed)Calculated dynamically within the correlation engine, the dynamic count of the number of exception work items that have been created as a result of this data quality issue. Helpful in determining whether any additional exceptions have been created so far. Note that rules order does not matter.CE[EWI.ID1, EWI.ID2,…]Exception Work Information ($EWI) Data Design for Correlation EngineThe correlation engine generates new work items in Exception Work Management Systems.  This section describes a minimal internal data model for managing Work Item Information within the Correlation Engine.  This is included solely to communicate concepts and document the sample rules below. The exact complete payload definition will be finalized as part of the Exception Work Management Project(s) and Exception Work Management Integrations Engine Integrations. These fields become implicit variables available within the Correlation Rules syntax.FieldDescription & PurposeSourceExample ValueComments1IDUnique identifier for this EWI within the correlation engine.  Must be globally unique and immutable across multiple data quality engines.  Used for deduplication and audit trail linkage.CEDQI-20260108-SAP-INV-001-5847Globally Unique Across all Work Engines2ExternalIDThe Workflow ID of the Exception Work Management ticket/task/workflow within the external system.EMEJIRA-10013TimestampTimestamp when the EWI was last synchronized with the receiving Exception Work management engine.EME2026-01-08T14:30:45.123Z4StatusThe NORMALISED state of the work item, used for correlation logic.PENDING: Not yet sent to Exception Management EngineNEW: Created within the Exception Management Engine, Work Not StartedIN PROGRESS: In Progress within the Work management EngineRESOLVED: Marked as manually fixed by a userVALIDATED: Machine confirmed as now corrected from the DQ EnginesCANCELLED: Cancelled as no longer required within the Work Management EngineCE (Mapped)IN PROGRESSImplies the ability to obtain current information from EME as needed. Also, the integration requirement is to map status to the normalised model.5RawStatusThe current raw state, direct from the EME, is unique to that platform.EMEWaiting for Vendor6PriorityThe NORMALISED priority of the work item.LOW, MEDIUM, HIGH, CRITICALCE (Mapped)CRITICAL7RawPriorityThe current raw priority is sourced directly from the EME and is unique to that platform.EMEP1 - Sev A8AssignmentGroupThe raw assignment data is directly from the EME and is unique to that platform.EMEData-Ops-Finance9CERuleIDThe Correlation Engine Rule that triggered the creation of this work item.CERULE-AGG-00510LinkageTypeIndicates whether this is a single-issue ticket or an aggregate ticket.SINGLE, AGGREGATE, DEPENDENCY_PARENTCEAGGREGATE11LinkedDQIssuesAn array of associated DQIssue IDs. Crucial for understanding which single work item handles each issue.CE[DQI.ID1, DQI.ID2,…]12PrimaryDQIssueThe ID of the first Data Quality Issue that triggered this work item, maintaining the root cause link.CEDQI-SAP-INV-00113MatchKeyThe Hashed matchkey associated with the ticket. Used in Correlation14TargetSystemIDThe ID of the specific Exception Management System instance where this item lives.CE/ConfigPROD-JIRA-01Audit Trail and Logging RequirementsThe Correlation Engine must maintain a complete, auditable record of all data-quality issue processing activities to support compliance, operational troubleshooting, and business analytics and rule effectiveness. This includes capturing the whole lineage from incoming DQ Issue through rule evaluation to work item creation or suppression. All audit data must be captured in real time and ingested into the Analytics Data Warehouse for trend and root-cause analysis and executive reporting.DQ Issue Receipt & Validation - Each incoming Data Quality Issue must be logged at receiptRule Evaluation & Matching - For each DQI, the engine must log rule evaluation results for each rule and its outcome.Rule Action Execution & Work Item Creation - Within a matched rule, log each resulting action, including any resulting work items created.Implied Data Quality Lifecycle DesignTo manage the Data Quality Issue Lifecycle status for all incoming Data Quality issues, as described above, we do not assume that all Data Quality Engines publish PASS events in follow-up to previous FAILED events.  As such, the Data Correlation Engine must infer the consequent pass status to maintain lifecycle synchronization with the work management engine.If Data Quality Engines do not send subsequent PASS events, the correlation engine will infer PASS based on the absence of the same issue in a future run/job.Logical Rule DesignFor the purposes of establishing an initial product design, listed below are broad categories of rules that the correlation engine is expected to support ultimately.  They are included here to aid the technical design and scoping process rather than as an absolute final list of business rule requirements.  Final business rules will be defined by the individual business teams that define data quality and response requirements. The aim is to develop a flexible, rules-based approach that can support an infinite number of distinct business requirements.We would expect capabilities to evolve, with simpler rule capabilities delivered first; however, the exact implementation sequence will depend on business priorities and technical implementation plans.Correlation Rule LanguageFor this document and communicating the concept of defining logical correlation rules, we have established a baseline correlation rule language below.The final rules specification language, syntax, and form will be defined during technical implementation,but are expected to follow a similarly extensible model.Ultimately, the correlation between language choice and structure will be based onHuman-readable and authorable by data management professionals and business users (not limited to trained technology staff)Syntactically flexible and adaptable both in its current scope and future potential.Syntactically checked and typed to ensure “correctness” through the editing process - structural errors should be picked up at the point of authoring, not at execution time.Testable ahead of time as part of the authoring process, so authors can understand the outcome of the rules they are creatingConsistent with other data governance rules implementation - i.e., Data Quality Rules, Data Conversion/Transformation Rules, etc.Rule Ordering, Parallelism, and ScalingTo address the likely need for high-volume scalability, we assume the Correlation Engine should be designed for parallel execution where rule order is generally considered non-deterministic. We assume that individual rules are mutually independent and can be evaluated concurrently. However, to ensure accurate event-level metrics and consistent state management, we enforce “processing atomicity per Data Quality Issue,” where all rules are processed against a single issue within a single instance. This ensures that, as the system scales horizontally across multiple issues, all rules for a single issue are evaluated within a single logical processing instance.In addition, we incorporate the requirement to enable a “Rule of Last Resort” capability that runs last after all other rules execute, only if no other rules have matched, and implicitly occurs after parallel evaluation is complete. Note that the design below assumes a single default rule across the entire environment.Stateful Processing and Action ExtensibilityA core design principle of the Correlation Engine is the extensibility of its actions, including the ability tounderstand events that have (recently) preceded the current event being processed and the current status of open work items,and use this information to determine the required action to take. The rule syntax is not limited to a single, stateless outcome; it can also query the state of existing work items before deciding on an action.This approach enables the engine to evolve from simple work-item creation to more sophisticated orchestration. A rule can check whether a work item already exists, update it, suppress duplicates, or perform multiple operations (e.g., create a ticket and send a notification) within a single logical evaluation. The syntax explicitly supports multi-action blocks, nested IF logic, and a STOP command for early exit, ensuring deterministic control flow.Sample Correlation Rule Language SyntaxAgain, the syntax presented here, along with the rule samples below, is provided for illustration purposes only. The exact rule specification and language must be validated during implementation.wide1800Rule CategoryCategory DescriptionExampleExample Logical RuleAssumptions / Additional Comments1Default "Catch-All" RuleThe default catch-all rule ultimately becomes a "rule of last resort" that ensures no data-quality issue is missed. This can be implemented as the first MVP rule, which is then deprioritized as other rule categories become available. If an incoming DQI does not match any preceding rule, this final rule will catch it.Any data quality issue that another rule has not matched is automatically assigned to the central 'Data Governance Triage' team with a 'MEDIUM' priority for manual review.The DEFAULT statement is a special construct. While standard rules execute in parallel, DEFAULT rules are queued to execute in a final "cleanup" phase, triggering only if the DQI has not been matched by any other rule during the parallel phase.2Basic System-Based Routing RulesSimple Routing rules route any data-quality issue from a specific source system to a designated team, with a fixed priority, within a particular work management system.These should be considered the “baseline” for all additional rule categories.For any DQ Issue originating from the Customer Master system ('CUST_MASTER'), create a work item for the 'Customer Data Stewardship' team to investigate.3Field-Based RoutingSpecific rules that route issues based on the field name involved, regardless of the system.For any issue related to the 'Credit Score' field in the 'Loan Origination' system, create a critical priority work item for the 'Credit Risk Underwriting' team.4Content-Based RoutingContent-Based Routing and more complex route issues are based on the specific data-quality rule that failed and a value in the record.If a customer record fails the 'CUST-001' (AML/KYC Check) and the customer's country is 'USA', create a critical priority ticket for the 'US-Compliance' team.Assumes the correlation engine has access to the underlying data.   Impacts the ability to manage rules on “sensitive data” without classifying the correlation engine as also sensitive.5Threshold-Based Alerting:Threshold-Based Alerting: Triggers an alert only when the volume of a specific issue type exceeds a defined threshold within a time window. This reduces noise from low-level, sporadic issues.If more than 100 inventory records fail the 'INV-987' (Stock Level Sync) rule within one hour, notify the 'Inventory Management' business team of a potential systemic failure impacting fulfillment.100) THEN
    CREATE 
        IN 'work-engine'
        ASSIGNED 'Data-Ops-Inventory'
        PRIORITY 'HIGH'
        DETAILS "Threshold breached for rule INV-987. Over 100 issues failures in 1 hour."
END]]>6Suppression Rule:Suppression Rule: Prevents the creation of work items for known, low-impact issues, such as problems in legacy data that is not scheduled for remediation.Do not create any tickets for data quality issues found in the 'LEGACY-CRM' system for records created before 2020.7Implied Lineage CorrelationImplied Lineage Correlation correlates similar issues across different systems by using an alternate match key that excludes the System ID. This groups identical failures for the same business entity across the data landscape. This is an alternative approach to quickly infer the same issue on the same business data record without requiring full integration with the lineage information in the Data Inventory tool. See Formal Lineage PrioritizationIf Rule 'Check-DOB' fails for Record 'C123' in both 'CRM' (Source) and 'Billing' (Downstream), link the Billing workflow to the existing CRM ticket instead of creating a new duplicate ticket.Requires consistent Business Record IDs across systems. If IDs change (surrogate keys), this rule would require an ID translation service.8Record-Level Aggregation:Aggregates multiple separate data quality failures occurring on the same business record into a single work item. This prevents creating 10 tickets for a single "bad row" that has 10 bad fields.If Record 'C-100' fails 'Check-Name', 'Check-Address', and 'Check-Phone', only one work item is created for 'C-100' and updated with the additional errors.Requires the MATCHKEY capability to key the ticket to the Record rather than the specific Issue.9Rule-Level Aggregation (Systemic):Aggregates all distinct failures of a specific Data Quality Rule into a single master work item. This is used when tracking the systemic failure of a rule is more important than tracking individual bad records.If Rule 'Date-Format-Check' fails for 1,000 different records in the 'CRM' system, create a single ticket for "Systemic Failure: Date-Format-Check" instead of 1,000 individual remediation tasks.Use with caution: This hides the specific record IDs from the work item header, though they appear in the comments/audit trail.10Lineage-Based Prioritization:Lineage-Based Prioritization: Escalates an issue’s priority if the affected data element (identified by its Critical Data Element ID) is known to feed into a critical downstream system.If a data quality issue affects the 'Customer TIN' or 'Trade Notional Amount' fields, and this data is consumed by the 'Regulatory-Reporting-Platform', automatically upgrade the issue priority to 'CRITICAL'.11Real-time High-Value Alerting:Real-time High-Value Alerting: Triggers an immediate, high-priority notification to an on-call system (like PagerDuty) for specific, high-impact failures.If a transaction over $10,000 fails a fraud check ('TXN-005'), send an immediate page to the on-call Financial Crimes team.10000 AND $DQI.State = 'FAIL') THEN
    NOTIFY PagerSystem('OnCall-FinCrime')
        DETAILS "High-value transaction failed fraud check. Immediate review required."
END]]>12Automated Remediation Trigger:Automated Remediation Trigger: Initiates a separate automated process or workflow when an issue meets a high-confidence threshold for automatic correction.If the duplicate customer detection rule ('DUP-CUST-01') identifies a high-confidence match, automatically trigger the 'Auto-Merge-Customer' workflow.13Context-Aware Severity Override:Context-Aware Severity Override: Overrides a ticket’s default severity based on the business context at detection, such as a critical financial period.During the 'Month-End-Close' business process, elevate any data quality issue from SAP to a 'High' priority ticket.14Multi-Team Coordination with Dependency:Multi-Team Coordination with Dependency: Creates linked tickets for multiple teams when an issue points to a cross-system problem, establishing a parent-child relationship for coordinated resolution.If a referential integrity check fails between the master data system (MDM) and the CRM, create a parent ticket for the 'Master Data Governance' team to fix the source and a dependent child ticket for the 'Sales Operations' team to handle the downstream business impact.Integration DesignIntegration with Data Quality ToolsWe expand the correlation engine to receive events from Data Quality Engines in (near) real time as soon as they become available.  We expect DQ issue volumes to be substantial and require substantial inbound processing.Integrations will need to support the transformation and normalization of key elements such as State (ie, PASS or FAIL).Integration will need to handle the variability in how different engines report successful remediation through two distinct modes:Integrations act as "stateful" sources, sending explicitFAILevents on detection andPASSevents when remediation is confirmed during re-evaluation.To support legacy engines that only report failures, the integration accepts "Job Completion" signals, allowing the engine to infer that a missing issue in a new job run indicates a successful fix.Integration with Data Inventory and Data Lineage Metadata storesUnlike the high-volume event stream of DQ tools, we expect this integration to operate as a Reference Lookup Service. Upon receipt of an event, the engine queries the Inventory using the SystemID + FieldID to retrieve governance metadata (e.g., DataOwner, DataSteward, SensitivityClassification). To improve performance, it may be appropriate to implement a caching mechanism for repeated information requests.The integration must support graph-based queries to identify Downstream Impact and Upstream Root Cause. This enables the engine to determine if a failure impacts specificEnd Consumer Use CasesorBusiness Lines.The integration serves as the bridge between technical schemas and business concepts. It maps physical columns toLogical Data Elements(defined by the Business) andCritical Data Elements (CDEs), enabling rules to be applied to specific governance policies.Integration with Business Intelligence (BI) Tools: Provides summarized insights and visualizations for BI platforms such as Tableau or Power BI to support executive reporting.Integration with the Data Exception Work Management toolUnlike the inbound-only Data Quality integration, this integration requires a two-way integration in which the correlation engine must initiate work requests, receive work item identifiers, and track progress.The Correlation Engine assigns work to logical "Teams" (e.g.,Data Ops, Finance). The Integration Layer is responsible for resolving these logical teams to specific physical destinations (e.g.,Jira Project: FIN_OPSvsServiceNow Group: FIN_DATA_STEWARDS).The integration must maintain a translation map between the Engine'sNormalized Status(e.g.,OPEN,IN_PROGRESS,RESOLVED) and the platform-specificRaw Status(e.g.,ToDo,In Analysis,Done). This ensures correlation rules remain platform-agnostic.Reference & Linkage Management:The integration must support creating relationships between tickets to enableAggregationandDependencyrules (e.g., linking 10 child "Data Concern" tickets to 1 parent "Master Data Incident").Product RoadmapThis section outlines an initial implementation roadmap.  It is essential to recognize that this product can start very simple and increase in complexity over time. The exact implementation plan will ultimately be driven by technology implementation and individual business correlation requirements that are not factored in here.PhaseScope/ Expected Outcomes1Phase #1 - Correlation Engine MVPAbility to consumer Data Quality issues from at least one Data Quality Engine (or consolidated source)Ability to initiate (not track state) in at least one Exception Work management EngineAbility to write simple one-in-one-out correlation rules in a basic editor2Phase #2 - Correlation Engine - Two-Way Work Item IntegrationExpansion of Exception Management Engine Integration to support two-way integration.Expansion of rules syntax and engine to take into account the current work item state information within rules3…
